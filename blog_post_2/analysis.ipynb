{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignorance isn't Bliss (Blog Post 2)\n",
    "\n",
    "In this notebook, we investigate whether we can improve the fairness of a machine learning model by removing protected or sensitive attribute labels (i.e. attributes that the organisation responsible for the system does not intend to discriminate against because of societal norms, law or policy - for example, gender) from the data. Intuitively, one might expect this to make the model ignorant or unaware of gender, and therefore lead to more equitable outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Let’s look at the behaviour of a machine learning system on an illustrative data set. An appropriate set of unit-records have already been simulated using the code [here](https://github.com/gradientinstitute/demo_code/blob/master/blog_post_2/make_data.ipynb), so we simply need to parse the file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('unit_records.csv')\n",
    "n = data.shape[0]\n",
    "\n",
    "# And split into training and testing fractions\n",
    "train_frac = 0.8\n",
    "n_train = int(n * train_frac)\n",
    "train = slice(0, n_train)\n",
    "test = slice(n_train, n)\n",
    "\n",
    "# The training targets are whether applicants were hired or not\n",
    "hired = data.Hired.values == \"Yes\"\n",
    "\n",
    "# And the key features\n",
    "experience = data.Experience.values.astype(float)\n",
    "is_male = data.Gender.values == \"Male\"\n",
    "\n",
    "display(data.loc[[898, 2343, 2398, 5906, 9394]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality\n",
    "\n",
    "Now we can investigate the relationship between experience and the label (hired / not hired) in terms of distribution and correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Experience-hiring correlation: {:.2f}\".format( \n",
    "      np.corrcoef(experience[train], hired[train])[0,1]))\n",
    "\n",
    "binx = np.linspace(0, 20, 11)\n",
    "yi, _ = np.histogram(experience[train][~hired[train]], bins=binx)\n",
    "ye, _ = np.histogram(experience[train][hired[train]], bins=binx)\n",
    "binx = .5 * (binx[1:] + binx[:-1])\n",
    "\n",
    "pl.figure(figsize=(5,2.5), dpi=150)\n",
    "pl.xlabel(\"Experience\")\n",
    "pl.ylabel(\"Frequency\")\n",
    "pl.hist([experience[~hired], experience[hired]], bins=binx,\n",
    "        label=['Not Hired', 'Hired'],\n",
    "        color=['#d9544d', '#01a049'])\n",
    "pl.legend()\n",
    "pl.tight_layout()\n",
    "pl.savefig(\"hired_hist.svg\")\n",
    "pl.show()\n",
    "\n",
    "print(\"Gender-hiring correlation: {:.2f}\".format( \n",
    "      np.corrcoef(is_male[train], hired[train])[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A standard model\n",
    "\n",
    "Clearly there is a relationship here, so lets go ahead and build a fairly standard classification model based on a scikit learn logistic regressor. We will train on the first 80% of the data, and test on the last 20%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = np.array([experience, is_male]).T\n",
    "standard_model = LogisticRegression(class_weight='balanced')\n",
    "standard_model.fit(X_full[train], hired[train])\n",
    "select = standard_model.predict(X_full[test])\n",
    "\n",
    "checks = [\n",
    "    (\"Overall\", slice(None)),\n",
    "    (\"Males\", is_male[test]),\n",
    "    (\"Females\", ~is_male[test]),\n",
    "]\n",
    "\n",
    "# Check the accuracy\n",
    "print(\"Accuracy (Standard Model):\")\n",
    "\n",
    "def stack(*parts):\n",
    "    \"\"\"Draw a horizontally stacked bar graph.\"\"\"\n",
    "    colors = {\n",
    "        \"lred\": \"#FF9999\",  # pink\n",
    "        \"green\": \"#009900\",  # Green\n",
    "        \"red\": \"#BB0000\",  # Red\n",
    "        \"lgreen\": \"#88DD88\",  # Light green\n",
    "        \"lblue\": \"#9999FF\",  # Blue\n",
    "    }\n",
    "    pl.figure(dpi=150)  \n",
    "    last = 0\n",
    "    for name, col, vals in parts:\n",
    "        count = vals.mean()\n",
    "        pl.barh(0, count, .05, left=last, color=colors[col],\n",
    "               edgecolor='k')\n",
    "        pl.text(last + 0.1 * count, 0, name)\n",
    "        last += count\n",
    "    pl.axis('image')\n",
    "    pl.axis('off')\n",
    "    pl.tight_layout()\n",
    "\n",
    "    \n",
    "def check_accuracy(select):\n",
    "    \"\"\"Draw plots assessing sub-group accuracy.\"\"\"\n",
    "    for name, filt in checks:\n",
    "        hire = hired[test][filt]\n",
    "        pred = select[filt]\n",
    "        acc = np.mean(hire==pred)\n",
    "        stack(\n",
    "            (\"Correct\", \"lgreen\", (pred == hire)),\n",
    "            (\"Incorrect\", \"lred\", (pred != hire)),\n",
    "        )\n",
    "        pl.title(\"\\n{:>20s}: {:.0f}% Correct\".format(name, 100.*acc))\n",
    "        pl.show()\n",
    "        \n",
    "check_accuracy(select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard model: Fairness of outcomes\n",
    "\n",
    "We know that there is no inherent difference in suitability of males and females in the scenario. We controlled for this when we simulated the data, because gender and suitability were drawn from independent random distributions. We can therefore examine the fairness of outcomes in this case.\n",
    "\n",
    "The accuracy of the selections above looked promising. The model is even slightly more accurate on Females, the group that we were worried might be disadvantaged.\n",
    "\n",
    "However, if we look at the outcomes in terms of the gender fraction of applicants we see a different story:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_composition(select):\n",
    "    \"\"\"Visualise the gender composition of the selected cohort and the\n",
    "    total cohort.\"\"\"\n",
    "    n_selected = np.sum(select)\n",
    "    male = is_male[test]\n",
    "    stack(\n",
    "         (\"Male Applicant\", \"lblue\", male),\n",
    "         (\"Female\", \"lred\", ~male),\n",
    "    )\n",
    "    pl.title(\"Testing group  ({} applicants) is {:.0f}% male.\".format(\n",
    "        n - n_train, 100. * is_male[test].mean()))\n",
    "    pl.show()\n",
    "\n",
    "    male = is_male[test][select]\n",
    "    stack(\n",
    "         (\"Male Selection\", \"lblue\", male),\n",
    "         (\"Female\", \"lred\", ~male),\n",
    "    )\n",
    "    pl.title(\"Selected group ({} applicants) is {:.0f}% male.\".format(\n",
    "        n_selected, 100. * male.mean()))\n",
    "\n",
    "    pl.show()\n",
    "\n",
    "\n",
    "def check_recall(selection):\n",
    "    \"\"\"Visualise the recall of the hired cohort.\"\"\"\n",
    "    for name, filt in checks:\n",
    "        true = hired[test][filt].astype(bool)\n",
    "        pred = selection[filt]\n",
    "        recall = pred[true]\n",
    "        stack(\n",
    "            (\"Selected Hire\", \"green\", recall),\n",
    "            (\"Not Selected\", \"lgreen\", ~recall),\n",
    "        )\n",
    "        pl.title(\"{:>10s}: {:.0f}%\".format(name, 100.*np.mean(pred[true])))\n",
    "\n",
    "print(\"\\nComposition (Standard Model):\")\n",
    "check_composition(select)\n",
    "print(\"\\nRecall (Standard Model):\")\n",
    "check_recall(select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that we are selecting a disproportionately male cohort - clearly females are being disadvantaged.\n",
    "\n",
    "It’s possible that this discrepancy is due to a historical bias against women in the training labels and is not consistent with the company’s notion of fair outcomes. When there is bias in the training labels, a model’s predictions will perpetuate it because the model has no wider knowledge to discern actual suitability from discrimination.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We have also checked the recall - the fraction of historically employed individuals that the algorithm would select. While 100% recall doesn't neccessarily mean fair (as there is likely bias in the historical outcomes too), this result does tell us that the model would disadvantage women, recovering less of the \"good\" females than the \"good\" males.\n",
    "\n",
    "## Fairness through unawareness\n",
    "\n",
    "An intuitive approach that you might adopt in an attempt to remedy this issue (and an approach that we’ve commonly seen used in practice) is to remove the protected attributes from the data so the model is ignorant or “unaware” of race or gender. Let’s see what impact this has on the model.\n",
    "\n",
    "### Unaware model\n",
    "\n",
    "Let's train a model that is unaware of gender, by removing the `is_male` column from the data and running the same checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try the \"intuitive fix\" of deleting gender\n",
    "X_unaware = np.array([experience]).T # Remove gender from model\n",
    "unaware_model = LogisticRegression(class_weight='balanced')\n",
    "unaware_model.fit(X_unaware[train], hired[train])\n",
    "select_new = unaware_model.predict(X_unaware[test])\n",
    "\n",
    "# Whats the accuracy and recall now:\n",
    "print(\"Accuracy (unaware model):\")\n",
    "\n",
    "check_accuracy(select_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unaware model: Fairness\n",
    "\n",
    "We notice a drop in accuracy - by removing gender we have given the model less information to learn from. Let's investigate whether we have more fair outcomes in return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the gender balance for the blind regressor \n",
    "print(\"Selection composition (unaware model)\")\n",
    "check_composition(select_new)\n",
    "\n",
    "# This is related to the recall of eligible individuals\n",
    "print(\"\\nRecall (unaware model):\")\n",
    "check_recall(select_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So despite the intuition that removing the protected feature from the set would help remove discriminatory historical bias, we’ve actually made things considerably worse for women.\n",
    "\n",
    "## What Went Wrong?\n",
    "\n",
    "Our intuition has led us astray - the outcomes were much worse for women when we deleted gender from the data.\n",
    "\n",
    "### Is there a conflation of gender and experience?\n",
    "\n",
    "Firstly, look at the distribution of experience by gender:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the relationship between gender and experience:\n",
    "pl.figure(figsize=(5,2.5), dpi=150)\n",
    "#pl.title(\"Experience vs Gender\")\n",
    "pl.xlabel(\"Experience\")\n",
    "pl.ylabel(\"Frequency\")\n",
    "pl.hist(\n",
    "    [experience[~is_male], experience[is_male]],\n",
    "    bins=binx,\n",
    "    label=['Female',  'Male'],\n",
    "    color=['#fec615', '#49759c'])\n",
    "pl.legend()\n",
    "pl.tight_layout()\n",
    "pl.savefig(\"gender_hist.svg\")\n",
    "# pl.savefig(\"gender_hist.png\", dpi=200)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that men have a higher average experience than women. While experience and gender is a relatively unsurprising relationship, we would in general expect a different joint-distribution of covariates when a population is partitioned by gender, race, age or other factors. In many real datasets with rich feature sets, we would expect to predict the sensitive attributes with very high accuracy through correlations with the other features. In this case, how recoverable is gender?\n",
    "\n",
    "\n",
    "### Can we recover gender from the experience feature?\n",
    "\n",
    "\n",
    "Lets train another simple logistic regression classifier to find out. This time, the input is experience, and the target output is gender. We'll train the model on the same training group (80% of the data), and validate on the remaining 20%: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In fact, can we recover gender from experience?\n",
    "gender_clf = LogisticRegression(class_weight='balanced')\n",
    "yg = is_male.astype(int)\n",
    "gender_clf.fit(X_unaware[train], yg[train])\n",
    "prediction = gender_clf.predict(X_unaware[test])\n",
    "accuracy = (prediction == yg[test]).mean()\n",
    "print(\"Accuracy recovering protected attribute: {:.0f}%\".format(\n",
    "    100*accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recover gender - not as accurately as we can predict whether an applicant will be hired, but much better than a random guess.\n",
    "\n",
    "### Why was the standard model better?\n",
    "\n",
    "While not perfect, the standard model was actually leading to more equitable gender outcomes than the unaware model. Lets inspect its internal weights to see what was going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original model Weights:\", end=\"\\n\\n\\t\")\n",
    "print(\"Experience: {:.2f}\\n\\tIs_Male:   {:.2f}\".format(*standard_model.coef_.ravel()))\n",
    "print(\"\\tIntercept: {:.2f}\".format(standard_model.intercept_.ravel()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the equation used by the standard model to predict the probability that an applicant will be hired was given by:\n",
    "\n",
    "$$\\large \\phi ( 0.56 e - 1.03 m - 5.69 ),$$\n",
    "\n",
    "where $\\phi$ is the logistic sigmoid function, $e$ is the experience feature and $m$ is the `is_male` indicator feature. The negative weight on `is_male` indicates that male probability-of-selection was being *reduced* to compensate for the effect of their higher average experience. This is conceptually equivalent to having a different male model and female model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
